\documentclass{article}
% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020
% ready for submission
% \usepackage{neurips_2020}
% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}
% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{neurips_2020}
% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2020}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
%\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{tikz}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{mathabx}
\usepackage{natbib}
\usepackage{listings}
\usepackage{bbm}
\usepackage{caption}
\usepackage{float}
\usepackage{setspace}
\usetikzlibrary{patterns}
\title{Incentive Incompatibility of Logistic Regression}
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.
\author{}
\begin{document}
\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{conj}{Conjecture}
\newtheorem{algo}{Algorithm}
\newtheorem{obs}{Observation}
\newtheorem{clm}{Claim}
\theoremstyle{definition}
\newtheorem{df}{Definition}
\newtheorem{eg}{Example}
\newtheorem{asm}{Assumption}
\newtheorem{cond}{Condition}
\theoremstyle{remark}
\newtheorem{rmk}{Remark}
\maketitle \allowdisplaybreaks \raggedbottom
\begin{abstract}
We study the incentive compatibility problem of multi-class logistic regression. We provide a simple numerical example in which a strategic data provider has the incentive to misreport her private label to increase the classification probability of her true label. In particular, the model trained given her true label classifies her data point incorrectly, whereas the model trained given her misreported label classifies her data point correctly. We show that this incentive incompatibility problem disappears if the logistic model is estimated by minimizing the zero-one loss, and if a Bayes classifier estimated with maximum likelihood is used.
\newline \newline

\end{abstract}
























\section{Introduction} 
There are $n $ strategic agents each providing the label of one data point to the principal. The principal is the learner and builds a machine learning model based on the data points provided by the agents. An agent, $i $, has publicly known feature vector, $x_{i}$, and a private discrete label, $y_{i}$. The objective of the agent is to maximize the probability that her data point is labeled correctly by the principal's model, and the agent can choose to report $y^{\dagger}_{i}$ to achieve the objective, with the possibility of misreporting $y^{\dagger}_{i} \neq  y_{i}$. We say a dataset is incentive incompatible with respect to the learner, described by a parametric model, if at least one of the $n $ agents has the incentive to misreport.
\newline \newline
The following is the diagram showing a dataset that is incentive incompatible with respect to the multi-class logistic regression model. In the dataset, each of the $n  = 18$ agents, $i $, has a two dimensional feature vector and a private label can take on one of three values: "red", "green", or "blue".
\newline \newline

\begin{figure}[H] \centering \begin{tikzpicture} [scale = 1] 
\draw[thick] (0.0, 0.0) circle [radius = 3.0];
\draw[thick] (0.0, 0.0) -- (0.0, 3.0);
\draw[fill = blue, thick] (0.2, 1.0) circle [radius = 0.1];
\draw[fill = blue, thick] (0.2, 2.0) circle [radius = 0.1];
\draw[fill = blue, thick] (0.2, 3.0) circle [radius = 0.1];
\draw[fill = red, thick] (-0.2, 1.0) circle [radius = 0.1];
\draw[fill = red, thick] (-0.2, 2.0) circle [radius = 0.1];
\draw[fill = red, thick] (-0.2, 3.0) circle [radius = 0.1];
\draw[thick] (0.0, 0.0) -- (2.6, -1.5);
\draw[fill = blue, thick] (0.97, -0.33) circle [radius = 0.1];
\draw[fill = blue, thick] (1.83, -0.83) circle [radius = 0.1];
\draw[fill = blue, thick] (2.7, -1.33) circle [radius = 0.1];
\draw[fill = green, thick] (0.77, -0.67) circle [radius = 0.1];
\draw[fill = green, thick] (1.63, -1.17) circle [radius = 0.1];
\draw[fill = green, thick] (2.5, -1.67) circle [radius = 0.1];
\draw[thick] (0.0, 0.0) -- (-2.6, -1.5);
\draw[fill = red, thick] (-0.97, -0.33) circle [radius = 0.1];
\draw[fill = red, thick] (-1.83, -0.83) circle [radius = 0.1];
\draw[fill = red, thick] (-2.7, -1.33) circle [radius = 0.1];
\draw[fill = green, thick] (-0.77, -0.67) circle [radius = 0.1];
\draw[fill = red, thick] (-1.73, -1.27) rectangle (-1.53, -1.07);
\draw[fill = green, thick] (-2.5, -1.67) circle [radius = 0.1];
\end{tikzpicture} \captionof{figure}{Incentive Incompatible Example}\label{fig:ice}
\end{figure}
The $18$ points are located inside a unit circle, and each point is $0.004$ away from the three line segments through the origin that forms angles of $120$ degrees between them. There is one point, labeled by a square in the plot, that is on the "incorrect" side of the boundary. Suppose the point corresponds to the feature vector of an agent $i $ with private label "red", then truthfully reporting her label will lead to a multi-class logistic regression model that classifies her point as "green". The probability that this model classifies her point as "red" is $0.3290$. However, if the agent misreports her label as "blue", the resulting model classifies her point as "red" with probability $0.4966$. Therefore, by lying about her label, the agent can make the principal learn an incorrect model that classifies her point correctly and with a higher probability.
\newline \newline
However, this dataset is incentive compatible if zero-one loss is minimized when estimating the logistic parameters, and if the Bayes classifier is used with maximum likelihood estimation. In general, misreporting will always lead to a lower classification probability of the agent's true label for these classifiers.
\newline \newline
Previous work on mechanism design for machine learning with strategic data sources focus on designing robust algorithms to incentivize the data providers to report their private data truthfully. Their models mainly differ in the objective and the possible actions of the data providers (agents) and the machine learner (principal).
\newline \newline
The first group of papers focuses on principal-agent problems in which each agent's private data point is the agent's type that the agent cannot change. The only action the agents can take is whether to report their private information truthfully.
\begin{enumerate}
\item Some models assume the agents' feature vectors are public, but their labels are private. \citet*{perote2004strategy}, \citet*{chen2018strategyproof}, and \citet*{gast2013linear} focus on strategy-proof linear regression algorithms and introduced clockwise repeated median estimators, generalized resistant hyperplane estimators, and modified generalized linear squares estimators. \citet*{dekel2010incentive} investigates the general regression problem with empirical risk minimization and absolute value loss. All the previously mentioned papers assume the labels are continuous variables (regression problems), and \citet*{meir2012algorithms} assumes the labels are discrete variables (classification problems) and proposes a class of random dictator mechanisms.
\item Some models assume the agents' feature vectors are also private. \citet*{chen2019grinding} investigates such problems for linear regressions.
\item Other models do not distinguish between feature vectors and labels. Each agent has a private valuation. These problems are usually modeled as facility locations problems and the solution involves some variant of the Vickrey-Clarke-Groves or Meyerson auction. These include \citet*{dutting2017optimal}, \citet*{golowich2018deep}, \citet*{epasto2018incentive}, and \citet*{procaccia2009approximate}.
\end{enumerate}


The second group papers focus on moral-hazard problems in which each agent does not have a type but they can choose an action (with a cost) that affects the probability of obtaining the correct label. \citet*{richardsonprivately} focuses on the linear regression problem in this scenario, and \citet*{cai2015optimum} and \citet*{shah2016double} investigates the problem for more general machine learning problems. \citet*{mihailescu2010strategy} also discusses a similar problem for general machine learning algorithms.
\newline \newline
The last group of papers uses machine learning or robust statistics techniques without game-theoretic models. This group of papers include \citet*{dekel2009vox}, \citet*{dekel2009good}.
\newline \newline



\section{Logistic Regression} 

\subsection{Model and Example}
In this section, we assume the principal is training a multi-class logistic (softmax) regression. There are $n $ strategic agents each providing the label of one data point to the principal. An agent, $i $, with public feature vector, $x_{i} \in \mathbb{R}^{m}$, and private discrete label, $y_{i} \in \left\{1, 2, ..., k \right\}$, has the objective of maximizing the probability that her data point is labeled correctly by the principal's model, parameterized by the $m  \times \left(k  + 1\right)$ weights (and bias) matrix $w $. The agent can choose to report $y^{\dagger}_{i}$ to achieve the objective, with possibly $y^{\dagger}_{i} \neq  y_{i}$. Denoting the weights of the model resulting from the false report from agent $i $ by $w^\star \left(y^{\dagger}_{i}\right)$, the agent's objective can be written as,
\begin{align*}
&  \displaystyle\max_{y^{\dagger} \in \left\{1, 2, ..., k \right\}} \mathbb{P}\left\{Y = y_{i} | w^\star \left(y^{\dagger}_{i}\right), x_{i}\right\},
\end{align*}
where,
\begin{align*}
&  \mathbb{P}\left\{Y = c | w, x_{i}\right\} = \dfrac{e^{z_{i,c}}}{\displaystyle\sum_{c'=1}^{k} e^{z_{i,c'}}},
\\ &  z_{i,c} = \displaystyle\sum_{j=1}^{m} w_{j,c} x_{i,j} + b_{c} , \text{\;for\;} c \in \left\{1, 2, ..., k\right\}.
\end{align*}
The principal is not strategic and he maximizes the likelihood of the data,
\begin{align*}
&\displaystyle\max_{w} \displaystyle\sum_{i=1}^{n} \log\left(\mathbb{P}\left\{Y = y^{\dagger}_{i} | w, x_{i}\right\}\right).
\end{align*}
We consider the case without a coalition of a group of agents, so only one agent is misreporting at a time, and use the following notations,
\begin{align*}
w^\star  &= \arg\displaystyle\max_{w} \displaystyle\sum_{i=1}^{n} \log\left(\mathbb{P}\left\{Y = y_{i} | w, x_{i}\right\}\right)
\\ w^\star \left(y^{\dagger}_{i}\right) &= \arg\displaystyle\max_{w} \log\left(\mathbb{P}\left\{Y = y^{\dagger}_{i} | w, x_{i}\right) + \displaystyle\sum_{i' = 0, i' \neq  i}^{n} \log\left(\mathbb{P}\left\{Y = y_{i'} | w, x_{i'}\right\}\right),\right.
\end{align*}
\begin{df} \label{df:ic} 
A dataset is incentive incompatible with respect to a learner if there exists at least one agent $i $, and some $y^{\dagger}_{i} \neq  y_{i}$ such that,
\begin{align*}
\mathbb{P}\left\{Y = y_{i} | w^\star , x_{i}\right\} &< \mathbb{P}\left\{Y = y_{i} | w^\star \left(y^{\dagger}_{i}\right), x_{i}\right\}.
\end{align*}
A learner (algorithm) is incentive compatible if there does not exist a dataset that is incentive incompatible.
\newline \newline\end{df}
\begin{prop} \label{prop:logit} 
Multi-class logistic regression is not incentive compatible.
\newline \newline\end{prop}

\begin{figure}[H] \centering \begin{tikzpicture} [scale = 1] 
\draw[thick] (-4.0, 0.0) -- (4.0, 0.0);
\draw[fill = green, thick] (-1.8, 0.1) circle [radius = 0.1];
\draw[fill = green, thick] (-1.8, 0.3) circle [radius = 0.1];
\draw[fill = green, thick] (-1.8, 0.5) circle [radius = 0.1];
\node[green, below right] at (-1.8, -0.1){-$0.49$};
\draw[fill = red, thick] (-2.2, 0.1) circle [radius = 0.1];
\draw[fill = red, thick] (-2.2, 0.3) circle [radius = 0.1];
\draw[fill = red, thick] (-2.2, 0.5) circle [radius = 0.1];
\node[red, below left] at (-2.2, -0.1){-$0.51$};
\draw[fill = green, thick] (-2.4, 0.4) rectangle (-2.6, 0.2);
\node[green, above left] at (-2.5, 0.5){-$0.53$};
\draw[fill = blue, thick] (3.0, 0.1) circle [radius = 0.1];
\draw[fill = blue, thick] (3.0, 0.3) circle [radius = 0.1];
\draw[fill = blue, thick] (3.0, 0.5) circle [radius = 0.1];
\node[blue, below] at (3.0, -0.1){$1$};
\end{tikzpicture} \captionof{figure}{Second Incentive Incompatible Example}\label{fig:sice}
\end{figure}
\begin{proof} \label{proof:logitpf} 
The simplest numerical example we found is the dataset with $10$ points $\left\{\left(x_{i}, y_{i}\right)\right\}_{i=1}^{10} = \left\{\left(-0.53, 2\right), \left(-0.51, 1\right), \left(-0.51, 1\right), \left(-0.51, 1\right), \left(-0.49, 2\right), \left(-0.49, 2\right), \left(-0.49, 2\right), \left(1, 3\right), \left(1, 3\right), \left(1, 3\right)\right\}$. It has a similar structure as the data set shown in Figure \ref{fig:ice} in that multiple points are close to a classification boundary, and the one point on the "incorrect" side of the boundary has the incentive to pretend to be the third class.
\\* In this example, agent $i $ reports $x_{i} \in \mathbb{R}^{2}$ and $y_{i}$ is one of $1$ (red), $2$ (green), or $3$ (blue). Suppose the green square point correspond to agent $1$ with $x_{1} = \left(-0.53, 2\right)$ and $y_{1} = 2$.
\begin{align*}
\mathbb{P}\left\{Y = 1 | w^\star , x_{1}\right\} &= 0.2715,
\\ \mathbb{P}\left\{Y = 1 | w^\star \left(y^{\dagger}_{1} = 3\right), x_{1}\right\} &= 0.3709.
\end{align*}
Here, $w^\star $ is given by, with class $1$ weights normalized to $0$,

\begin{center} \begin{tabular}{|c|c|c|c|}
\hline
 Class &(Intercept) &x1\\ \hline
$2$ &$25.73034$ &$50.40985$\\ \hline
$3$ &$21.77434$ &$63.65515$\\ \hline
\end{tabular} \end{center}
and $w^\star \left(y^{\dagger}_{1} = 3\right)$ is given by,

\begin{center} \begin{tabular}{|c|c|c|c|}
\hline
 Class &(Intercept) &x1\\ \hline
$2$ &$6.603135$ &$13.09844$\\ \hline
$3$ &$8.231013$ &$18.48809$\\ \hline
\end{tabular} \end{center}
It turns out that the incentive incompatibility heavily depends the numerical stability of the weights-finding algorithm: using BFGS to maximize the likelihood shows that this dataset is actually incentive compatible for all points.
\newline \newline\end{proof}
The example described in Figure \ref{fig:ice} is a dataset that is also incentive incompatible.
\\* In this example, agent $i $ reports $x_{i} \in \mathbb{R}^{2}$ and $y_{i}$ is one of $1$ (red), $2$ (green), or $3$ (blue). Suppose the red square point correspond to agent $1$ with $x_{1} = \left(-1.63, -1.17\right)$ and $y_{1} = 1$.
\begin{align*}
\mathbb{P}\left\{Y = 1 | w^\star , x_{1}\right\} &= 0.3290,
\\ \mathbb{P}\left\{Y = 1 | w^\star \left(y^{\dagger}_{1} = 3\right), x_{1}\right\} &= 0.4966.
\end{align*}
Here, parameter estimation is done using maximum likelihood estimation with BFGS, and $w^\star $ is given by, with class $1$ weights normalized to $0$,

\begin{center} \begin{tabular}{|c|c|c|c|c|}
\hline
 Class &(Intercept) &x1 &x2\\ \hline
$2$ &-$0.6053178$ &$104.9925$ &-$181.3391914$\\ \hline
$3$ &-$0.2852057$ &$209.4190$ &$0.3656777$\\ \hline
\end{tabular} \end{center}
and $w^\star \left(y^{\dagger}_{1} = 3\right)$ is given by,

\begin{center} \begin{tabular}{|c|c|c|c|c|}
\hline
 Class &(Intercept) &x1 &x2\\ \hline
$2$ &-$0.1915645$ &$3.473426$ &-$5.507418$\\ \hline
$3$ &$0.8273350$ &$4.309293$ &-$1.200060$\\ \hline
\end{tabular} \end{center}
Using BFGS to maximize likelihood leads to the same incentive incompatibility result. Currently there is no formal proof that the result is not due to numerical instability.
\newline \newline


\subsection{Binary Classification}
\begin{prop} \label{prop:bin} 
Binary classification using ERM with any loss function $\ell$ is incentive compatible.
\end{prop}
\begin{proof} \label{proof:binpf} 
For any dataset $\left\{\left(x_{i}, y_{i}\right)\right\}_{i=1}^{n}$, and the hypothesis class $\mathcal{H}$, let the optimal classifier in the case every agent report truthfully be,
\begin{align*}
h^\star  &= \arg\displaystyle\min_{h \in \mathcal{H}} \displaystyle\sum_{i'=1}^{n} \ell\left(h, x_{i'}, y_{i'}\right).
\end{align*}
Fix an agent $i $, her feature vector $x_{i}$, and fix other agents' reports, $\left(x_{-i}, y_{-i}\right)$, define the optimal classifier given the classifier $h $ and the misreport of agent $i , y^{\dagger}_{i} = 1 - y_{i}$ as,
\begin{align*}
h^\star \left(y^{\dagger}_{i}\right) &= \arg\displaystyle\min_{h \in \mathcal{H}} \displaystyle\sum_{i'=1, i' \neq  i}^{n} \ell\left(h, x_{i'}, y_{i'}\right) + \ell\left(h, x_{i}, y^{\dagger}_{i}\right).
\end{align*}
Now suppose, for a contradiction, that agent $i $ prefers misreporting,
\begin{align*}
\ell\left(h^\star , x_{i}, y_{i}\right) &> \ell\left(h^\star \left(y^{\dagger}_{i}\right), x_{i}, y_{i}\right),
\end{align*}
which implies, since $y^{\dagger}_{i} = 1 - y_{i,}$
\begin{align*}
\ell\left(h^\star , x_{i}, y^{\dagger}_{i}\right) &< \ell\left(h^\star \left(y^{\dagger}_{i}\right), x_{i}, y^{\dagger}_{i}\right).
\end{align*}
Note that the above implication only works for binary classification.
\\* Due to the optimality of $h^\star \left(y^{\dagger}_{i}\right)$,
\begin{align*}
\displaystyle\sum_{i'=1, i' \neq  i}^{n} \ell\left(h^\star \left(y^{\dagger}_{i}\right), x_{i'}, y_{i'}\right) + \ell\left(h^\star \left(y^{\dagger}_{i}\right), x_{i}, y^{\dagger}_{i}\right) &\leq  \displaystyle\sum_{i'=1, i' \neq  i}^{n} \ell\left(h^\star , x_{i'}, y_{i'}\right) + \ell\left(h^\star , x_{i}, y^{\dagger}_{i}\right),
\end{align*}
using the above inequalities, the comparison can be simplified to,
\begin{align*}
\displaystyle\sum_{i'=1, i' \neq  i}^{n} \ell\left(h^\star \left(y^{\dagger}_{i}\right), x_{i'}, y_{i'}\right) &\leq  \displaystyle\sum_{i'=1, i' \neq  i}^{n} \ell\left(h^\star , x_{i'}, y_{i'}\right),
\\ \displaystyle\sum_{i'=1}^{n} \ell\left(h^\star \left(y^{\dagger}_{i}\right), x_{i'}, y_{i'}\right) &\leq  \displaystyle\sum_{i'=1}^{n} \ell\left(h^\star , x_{i'}, y_{i'}\right),
\end{align*}
which is a contradiction to the optimality of $h^\star $.
\newline \newline\end{proof}


\subsection{Zero-One Loss Logistic Regression}
It is, however, possible to change the loss function so that logistic regression is incentive compatible. Changing the loss function to absolute value $L^{1}$ loss is one possibility, due to \citet*{dekel2010incentive}. Their result on incentive compatibility of empirical risk minimization in the regression setting is applicable in our model. In addition to absolute value loss, which is not a meaningful loss function for multi-class logistic regression, zero-one loss logistic regression with deterministic predictions is also incentive compatible.
\newline \newline
\begin{prop} \label{prop:zolog} 
Multi-class deterministic classifiers estimated by empirical risk minimization with zero-one loss is incentive compatible.
\end{prop}
\begin{proof} \label{proof:zologpf} 
For any dataset $\left\{\left(x_{i}, y_{i}\right)\right\}_{i=1}^{n}$, and the hypothesis class $\mathcal{H}$, let the optimal classifier be,
\begin{align*}
h^\star  &= \arg\displaystyle\min_{h \in \mathcal{H}} \displaystyle\sum_{i'=1}^{n} \mathbbm{1}_{\left\{y_{i'} \neq  h\left(x_{i'}\right)\right\}}.
\end{align*}
Fix an agent $i $, her feature vector $x_{i}$, and fix other agents' reports, $\left(x_{-i}, y_{-i}\right)$, define the loss function given the classifier $h $ and report of agent $i , y^{\dagger}_{i}$ as,
\begin{align*}
\mathcal{L}\left(h, y^{\dagger}_{i}\right) &= \displaystyle\sum_{i' \neq  i} \mathbbm{1}_{\left\{y_{i'} \neq  h\left(x_{i'}\right)\right\}} + \mathbbm{1}_{\left\{y^{\dagger}_{i} \neq  h\left(x_{i}\right)\right\}}.
\end{align*}
If $y_{i} = h^\star \left(x_{i}\right)$, then the classifier is already classifying $x_{i}$ correctly, misreporting will not improve the outcome for $i $. Now let the prediction be $h^\star \left(x_{i}\right) = y^\star  \neq  y_{i}$, and suppose $h^\star $ is making $k $ mistakes, meaning,
\begin{align*}
k  &= \displaystyle\min_{h \in \mathcal{H}} \mathcal{L}\left(h^\star , y_{i}\right).
\end{align*}
Agent $i $ can misreport in the following two ways:
\end{proof}
\begin{enumerate}
\item If agent $i $ reports $y^{\dagger}_{i} = y^\star $, let the new classifier be $h^{\dagger}$, note that we must have,
\begin{align*}
\mathcal{L}\left(h^{\dagger}, y^\star \right) &\leq  k - 1,
\end{align*}
because $\mathcal{L}\left(h^{\dagger}, y^\star \right) > k - 1 = \mathcal{L}\left(h^\star , y^\star \right)$ contradicts the optimality of $h^{\dagger.}$
\\* Now suppose that agent $i $ could get her true label with $h^{\dagger}$, meaning $h^{\dagger}\left(x_{i}\right) = y_{i}$, then,
\begin{align*}
\mathcal{L}\left(h^{\dagger}, y_{i}\right) &= \mathcal{L}\left(h^{\dagger}, y^\star \right) - 1
\\ &\leq  k - 2
\\ &< \mathcal{L}\left(h^\star , y_{i}\right),
\end{align*}
which contradicts the optimality of $h^\star $. Therefore, agent $i $ cannot improve the outcome by misreporting $y^\star $.
\item If agent $i $ reports $y^{\dagger}_{i} = y' \neq  y^\star $, let the new classifier be $h^{\dagger}$, note that we must have,
\begin{align*}
\mathcal{L}\left(h^{\dagger}, y'\right) &\leq  k,
\end{align*}
because if $\mathcal{L}\left(h^{\dagger}, y'\right) > k = \mathcal{L}\left(h^\star , y'\right)$ contradicts the optimality of $h^{\dagger.}$
\\* Now suppose that agent $i $ could get her true label with $h^{\dagger}$, then,
\begin{align*}
\mathcal{L}\left(h^{\dagger}, y_{i}\right) &= \mathcal{L}\left(h^{\dagger}, y_{i}\right) - 1
\\ &\leq  k - 1
\\ &< \mathcal{L}\left(h^\star , y_{i}\right),
\end{align*}
which contradicts the optimality of $h^\star $. Therefore, agent $i $ cannot improve the outcome by misreporting $y'$.
\\* Therefore, no agent can improve the outcome and the dataset is incentive compatible.
\newline \newline
\end{enumerate}




\section{Bayes Classifier} 
The example given previously is incentive compatible with respect to the Naive Bayes classifier. None of the agents have the incentive to misreport their labels. This is always true in general for any parametric Bayesian classifier estimated using maximum likelihood.
\newline \newline
\begin{prop} \label{prop:bc} 
Bayesian classifiers are incentive compatible.
\end{prop}
\begin{proof} \label{proof:bcpf} 
Suppose the loglikelihood function of class $y $ given the feature vector $x $ and the parameter $w $ is $\ell\left(x ; w \right)$, and define the optimal parameter, $w^\star $ for class $y_{i}$, of the truthful model as,
\begin{align*}
w^\star  &= \arg\displaystyle\max_{w} \displaystyle\sum_{i' : y_{i'} = y_{i}} \ell\left(x_{i'} ; w\right)
\\ &= \arg\displaystyle\max_{w} \displaystyle\sum_{i' \neq  i : y_{i'} = y_{i}} \ell\left(x_{i'} ; w\right) + \ell\left(x_{i} ; w\right).
\end{align*}
Let the optimal parameter when agent $i $ reports $y^{\dagger}_{i} \neq  y_{i}$ be $w^{\dagger,}$
\begin{align*}
w^{\dagger} &= \arg\displaystyle\max_{w} \displaystyle\sum_{i' \neq  i: y_{i'} = y_{i}} \ell\left(x_{i'} ; w\right).
\end{align*}
In particular, these implies the following optimality conditions,
\begin{align*}
\displaystyle\sum_{i' \neq  i : y_{i'} = y_{i}} \ell\left(x_{i'} ; w^\star \right) + \ell\left(x_{i} ; w^\star \right) &\geq  \displaystyle\sum_{i' \neq  i : y_{i'} = y_{i}} \ell\left(x_{i'} ; w^{\dagger}\right) + \ell\left(x_{i} ; w^{\dagger}\right),
\\ \displaystyle\sum_{i' \neq  i: y_{i'} = y_{i}} \ell\left(x_{i'} ; w^{\dagger}\right) &\geq  \displaystyle\sum_{i' \neq  i: y_{i'} = y_{i}} \ell\left(x_{i'}, w^\star \right).
\end{align*}
Taking the difference between the two inequalities, we have,
\begin{align*}
\ell\left(x_{i} ; w^\star \right) &\geq  \ell\left(x_{i} ; w^{\dagger}\right).
\end{align*}
Note that the empirical prior probability for class $y_{i}$ is decreased if the number of data with label $y_{i}$ is decreased by $1$. Therefore, the posterior probabilities satisfy,
\begin{align*}
\mathbb{P}\left\{y_{i} | x_{i}, w^\star \right\} &\geq  \mathbb{P}\left\{y_{i} | x_{i}, w^{\dagger}\right\}.
\end{align*}
Therefore, no agent can improve the outcome and the dataset is incentive compatible.
\newline \newline\end{proof}
One special case of this is the Bayes classifier for the Gaussian Mixture Model.
\newline \newline



\section{Kernel Density Estimators} 
There are two general approaches to use kernel densities for classification, the first is to use all the points to estimate the density and the second is to estimate the densities for each class separately (see \citet*{taylor1997classification}).
\newline \newline
The first approach suggests that,
\begin{align*}
\mathbb{P}\left\{X = x\right\} &= \dfrac{1}{n h^{D}} \displaystyle\sum_{i'=1}^{n} w_{i'}, \text{\;where\;} w_{i'} = K\left(\dfrac{x - x_{i'}}{h}\right).
\end{align*}
Define $w^\star $ to be the weight function when all agents report truthfully, then the classification probabilities for agent $i $ from dividing up the sum based on the class is,
\begin{align*}
\mathbb{P}\left\{Y = y_{i}, X = x_{i} | w^\star \right\} &= \dfrac{1}{n} \displaystyle\sum_{i'=1}^{n} w^\star _{i'} \mathbbm{1}_{\hat{y}_{i'} = y_{i}}
\\ &= \dfrac{1}{n} \displaystyle\sum_{i'=1}^{n} w^\star _{i'} \mathbbm{1}_{\hat{y}_{i'} = y_{i}, i' \neq  i} + \dfrac{1}{n} w^\star _{i}
\\ &= \mathbb{P}\left\{Y = y_{i}, X = x_{i} | w^\star \left(y^{\dagger}_{i}\right)\right\} + \dfrac{1}{n} K\left(0\right), \text{\;since\;} y^{\dagger}_{i} \neq  y_{i}
\\ &\leq  \mathbb{P}\left\{Y = y_{i}, X = x_{i} | w^\star \left(y^{\dagger}_{i}\right)\right\},
\end{align*}
meaning reporting truthfully results in a larger probability compared to reporting $y^{\dagger}_{i}$ instead.
\newline \newline
Alternative if the classification probabilities are computed based on \citet*{taylor1997classification},
\begin{align*}
\mathbb{P}\left\{X = x | Y = y\right\} &= \dfrac{1}{n_{y} h^{D}} \displaystyle\sum_{i'=1}^{n} w_{i'} \mathbbm{1}_{\hat{y}_{i'} = y}, n_{y} = \displaystyle\sum_{i'=1}^{n} \mathbbm{1}_{\hat{y}_{i'} = y}
\end{align*}
Similar to the above derivation (and also as a special case of a Bayes estimator),
\begin{align*}
\mathbb{P}\left\{Y = y_{i}, X = x_{i} | w^\star \right\} &= \dfrac{1}{n_{y}} \displaystyle\sum_{i'=1}^{n} w^\star _{i'} \mathbbm{1}_{\hat{y}_{i'} = y_{i}}
\\ &= \mathbb{P}\left\{Y = y_{i}, X = x_{i} | w^\star \left(y^{\dagger}_{i}\right)\right\} + \dfrac{1}{n_{y}} K\left(0\right)
\\ &\leq  \mathbb{P}\left\{Y = y_{i}, X = x_{i} | w^\star \left(y^{\dagger}_{i}\right)\right\}.
\end{align*}
K-Nearest Neighbor is a special case of this with a uniform kernel.
\newline \newline



\section{Artificial Examples} 
One intuition behind why the above classifiers are always incentive-compatible is that one-vs-one classification decisions are made independently for classifiers. The classifiers like logistic regression have highly interdependent one-vs-one decisions. The following example is one in which $1$-vs-$2$ decisions are completely determined by the $2$-vs-$3$ decisions, and as a result, a class-$1$ point that is misclassified as class-$2$ could misreport as class-$3$ to influence the $2$-vs-$3$ decision boundary and indirectly change the $1$-vs-$2$ decision boundary in its favor.
\newline \newline
Consider the three-class $1D$ threshold classifiers in the form,
\[ \hat{y}\left(x; t\right) =\left\{ \begin{array}{ll}
1& \text{\;if\;} x < -t \\
2& \text{\;if\;} -t \leq  x \leq  t \\
3& \text{\;if\;} x > t \\
\end{array}\right. \]
This is effectively a one-vs-one threshold classifier with the $1$-vs-$2$ threshold of $-t $, with the $2$-vs-$3$ threshold of $t $ and the $1$-vs-$3$ threshold of any value between $-t $ and $t $. Now define the loss function as the error margin,
\[ \ell\left(x_{i}, y_{i}; t\right) =\left\{ \begin{array}{ll}
\left(x_{i} - \left(-t\right)\right)^{2} \mathbbm{1}_{\hat{y}\left(x_{i}; t\right) \neq  y_{i}}& \text{\;if\;} y_{i} = 1 \\
\displaystyle\min\left\{\left(x_{i} - \left(-t\right)\right)^{2}, \left(x_{i} - t\right)^{2}\right\} \mathbbm{1}_{\hat{y}\left(x_{i}; t\right) \neq  y_{i}}& \text{\;if\;} y_{i} = 2 \\
\left(x_{i} - t\right)^{2} \mathbbm{1}_{\hat{y}\left(x_{i}; t\right) \neq  y_{i}}& \text{\;if\;} y_{i} = 3 \\
\end{array}\right. \]
Then, the learner's maximization problem is,
\begin{align*}
&\displaystyle\min_{t \in \mathbb{R}} \ell\left(x_{i}, y_{i}; t\right).
\end{align*}
The agents' problem is to minimize the loss for their point with their true label. The loss can be either the margin loss or zero-one loss, here, for simplicity, assume the agents also want to minimize the margin,
\begin{align*}
&\displaystyle\max_{y^{\dagger}_{i} \in \left\{1, 2, 3\right\}} \ell\left(x_{i}, y_{i}; t^\star \left(y^{\dagger}_{i}\right)\right),
\end{align*}
where $t^\star \left(y^{\dagger}_{i}\right)$ is the optimal threshold $t $ for the learner when agent $i $ misreports the label as $y^{\dagger}_{i.}$
\\* For the dataset $\left\{\left(x_{i}, y_{i}\right)\right\}_{i=1}^{5} = \left\{\left(-4, 1\right), \left(-3, 2\right), \left(-2, 1\right), \left(3, 2\right), \left(4, 3\right)\right\}$, the minimum loss is $1$ and it occurs when $t  = 3$ when every agent reports truthfully. In this case, agent (-$2, 1$) is misclassified as (-$2, 2$). However, if (-$2, 1$) misreports as (-$2, 3$), the loss from $t  = 3$ is $25$ and the minimum loss is $22$ and it occurs when $t  = 0$. In this case, agent (-$2, 1$) is classified correctly. In particular, the agent (-$2, 1$) improves the loss from $1$ to $0$.
\newline \newline

\begin{figure}[H] \centering \begin{tikzpicture} [scale = 1] 
\draw[thick] (-4.0, 0.0) -- (4.0, 0.0);
\draw[fill = green, thick] (-4.0, 0.0) circle [radius = 0.1];
\draw[fill = green, thick] (-2.1, -0.1) rectangle (-1.9, 0.1);
\draw[fill = red, thick] (-3.0, 0.0) circle [radius = 0.1];
\draw[fill = red, thick] (3.0, 0.0) circle [radius = 0.1];
\draw[fill = blue, thick] (4.0, 0.0) circle [radius = 0.1];
\draw[green, thick] (-4.0, -0.2) -- (-3.0, -0.2);
\draw[red, thick] (-3.0, -0.2) -- (3.0, -0.2);
\draw[blue, thick] (3.0, -0.2) -- (4.0, -0.2);
\node[below] at (0.0, -0.2){$0$};
\end{tikzpicture} \captionof{figure}{$1D$ Artificial Incentive Incompatible Example (Truthful)}\label{fig:1dat}
\end{figure}

\begin{figure}[H] \centering \begin{tikzpicture} [scale = 1] 
\draw[thick] (-4.0, 0.0) -- (4.0, 0.0);
\draw[fill = green, thick] (-4.0, 0.0) circle [radius = 0.1];
\draw[fill = blue, thick] (-2.1, -0.1) rectangle (-1.9, 0.1);
\draw[fill = red, thick] (-3.0, 0.0) circle [radius = 0.1];
\draw[fill = red, thick] (3.0, 0.0) circle [radius = 0.1];
\draw[fill = blue, thick] (4.0, 0.0) circle [radius = 0.1];
\draw[green, thick] (-4.0, -0.2) -- (0.0, -0.2);
\draw[red, thick] (0.0, -0.2) -- (0.0, -0.2);
\draw[blue, thick] (0.0, -0.2) -- (4.0, -0.2);
\node[below] at (0.0, -0.2){$0$};
\end{tikzpicture} \captionof{figure}{$1D$ Artificial Incentive Incompatible Example (Misreport)}\label{fig:1dam}
\end{figure}
Intuitively, when the misclassified class-$1$ point pretends to be a class-$3$ point with a large loss, the classifier modifies the $2$-vs-$3$ decision boundary to minimize that loss and the point benefits from the interdependence between the $2$-vs-$3$ decision and the $1$-vs-$2$ decision.
\newline \newline
Consider another three-class $2D$ threshold classifier in the form,
\[ \hat{y}\left(x_{1}, x_{2}; a, b, c, d\right) =\left\{ \begin{array}{ll}
1& \text{\;if\;} a x_{1} + b x_{2} + c \leq  0 \text{\;and\;} b x_{1} - a x_{2} + d > 0 \\
2& \text{\;if\;} a x_{1} + b x_{2} + c \leq  0 \text{\;and\;} b x_{1} - a x_{2} + d \leq  0 \\
3& \text{\;if\;} a x_{1} + b x_{2} + c > 0 \\
\end{array}\right. \]
\begin{align*}
\ell\left(x_{i}, y_{i}; t\right) &= d\left(x_{i}, t\right) \mathbbm{1}_{\hat{y}\left(x_{i}; t\right) \neq  y_{i}}, t = \left(a, b, c, d\right) \in \mathbb{R}^{4}
\end{align*}
where $d\left(x_{i}, t \right) $ is the minimum distance from $x_{i}$ to one of the lines $a  x_{1} + b x_{2} + c = 0$ and $b  x_{1} - a x_{2} + d = 0$, or,
\begin{align*}
d\left(x_{i}, t \right)  &= \dfrac{\displaystyle\min\left(| a x_{1} + b x_{2} + c |, | b x_{1} - a x_{2} + d |\right)}{\sqrt{a^{2} + b^{2}}}.
\end{align*}
For the dataset $\left\{\left(x_{i1}, x_{i2}, y_{i}\right)\right\}_{i=1}^{5} = \left\{\left(0, 0, 1\right), \left(0, 0, 3\right), \left(0, -1, 1\right), \left(0, -1, 2\right), \left(1, 0, 2\right)\right\}$, the minimum loss is $0$ and it occurs when $a  = b  = c  = d  = 0$ when every agent reports truthfully. In this case, agent $\left(0, -1, 1\right)$ is misclassified as $\left(0, -1, 2\right)$. However, if $\left(0, -1, 1\right)$ misreports as $\left(0, -1, 3\right)$, the loss from $a  = b  = c  = d  = 0$ is $1$ and the minimum loss is $\dfrac{\sqrt{2}}{2}$ and it occurs when $a  = -1, b   = c  = d  = 1$. In this case, agent $\left(0, -1, 1\right)$ is classified correctly.
\newline \newline

\begin{figure}[H] \centering \begin{tikzpicture} [scale = 1] 
\draw[blue, thick] (-4.0, 0.1) -- (4.0, 0.1);
\draw[green, thick] (-4.0, 0.0) -- (-0.1, 0.0) -- (-0.1, -4.0);
\draw[red, thick] (4.0, 0.0) -- (0.0, 0.0) -- (0.0, -4.0);
\draw[fill = green, thick] (-0.1, 0.0) circle [radius = 0.1];
\draw[fill = red, thick] (0.0, -1.9) circle [radius = 0.1];
\draw[fill = green, thick] (-0.1, -2.2) rectangle (0.1, -2.0);
\draw[fill = red, thick] (1.9, 0.0) circle [radius = 0.1];
\draw[fill = blue, thick] (0.1, 0.1) circle [radius = 0.1];
\node[above right] at (0.0, 0.2){$\left(0, 0\right)$};
\end{tikzpicture} \captionof{figure}{$2D$ Artificial Incentive Incompatible Example (Truthful)}\label{fig:2dat}
\end{figure}

\begin{figure}[H] \centering \begin{tikzpicture} [scale = 1] 
\draw[blue, dashed, thick] (-4.0, 0.1) -- (4.0, 0.1);
\draw[green, dashed, thick] (-4.0, 0.0) -- (-0.1, 0.0) -- (-0.1, -4.0);
\draw[red, dashed, thick] (4.0, 0.0) -- (0.0, 0.0) -- (0.0, -4.0);
\draw[blue, thick] (2.0, 0.3) -- (-2.0, -3.7);
\draw[green, thick] (-2.0, -3.9) -- (-0.1, -2.0) -- (2.0, -4.1);
\draw[red, thick] (2.0, 0.1) -- (0.0, -1.9) -- (2.0, -3.9);
\draw[fill = green, thick] (-0.1, 0.0) circle [radius = 0.1];
\draw[fill = red, thick] (0.0, -1.9) circle [radius = 0.1];
\draw[fill = blue, thick] (-0.1, -2.2) rectangle (0.1, -2.0);
\draw[fill = red, thick] (1.9, 0.0) circle [radius = 0.1];
\draw[fill = blue, thick] (0.1, 0.1) circle [radius = 0.1];
\node[above right] at (0.0, 0.2){$\left(0, 0\right)$};
\end{tikzpicture} \captionof{figure}{$2D$ Artificial Incentive Incompatible Example (Misreport)}\label{fig:2dam}
\end{figure}
To generalize the above observations, suppose the learner is a probabilistic classifier with parameters estimated by maximum likelihood, and the agents report their labels to maximize the classification probability of their true labels. Then the following two conditions guarantee that the classification is incentive compatible.
\newline \newline
\begin{df} \label{df:mono} 
(Monotonic Condition) A multi-class probabilistic classifier is monotonic if, given a training set $S $, for any point $x $ with labels $a $ and $b, $
\begin{align*}
\dfrac{\mathbb{P}\left\{Y = a | x, w^\star \left(S\right)\right\}}{\mathbb{P}\left\{Y = b | x, w^\star \left(S\right)\right\}} &\geq  \dfrac{\mathbb{P}\left\{Y = a | x, w^\star \left(S \cup \left\{\left(x, y = a\right)\right\}\right)\right\}}{\mathbb{P}\left\{Y = b | x, w^\star \left(S \cup \left\{\left(x, y = a\right)\right\}\right)\right\}}.
\end{align*}\end{df}
The assumption says that the probability that $x $ is classified as $a $ increases when there is an additional point $\left(x , a \right)$ in the training set.
\newline \newline
\begin{df} \label{df:iia} 
(Independence of Irrelevant Alternatives (IIA) Condition) A multi-class classifier is independent of irrelevant alternatives if, given a training set $S $, for any point $x $ and any pair of labels $a $ and $b, $
\begin{align*}
\dfrac{\mathbb{P}\left\{Y = a | x, w^\star \left(S\right)\right\}}{\mathbb{P}\left\{Y = b | x, w^\star \left(S\right)\right\}} &= \dfrac{\mathbb{P}\left\{Y = a | x, w^\star \left(S \cup \left\{\left(x', y' \notin \left\{a, b\right\}\right)\right\}\right)\right\}}{\mathbb{P}\left\{Y = b | x, w^\star \left(S \cup \left\{\left(x', y' \notin \left\{a, b\right\}\right)\right\}\right)\right\}}.
\end{align*}\end{df}
The assumption says that the ratio between the classification probabilities of $x $ of any two classes is not changed by adding a point at $x $ with a third class.
\newline \newline
Combining the two assumptions MC and IIA, we have that an agent with label $a $ cannot change the decision of $a $ vs $b $ by misreporting its label as a third class $c $. This observation is formalized in the following proposition.
\newline \newline
\begin{prop} \label{prop:margin} 
A multi-class probabilistic classifier estimated by maximum likelihood is incentive compatible if it is monotonic and independent of irrelevant alternatives.
\end{prop}
\begin{proof} \label{proof:marginpf} 
Fix a dataset $\left\{\left(x_{i}, y_{i}\right)\right\}_{i=1}^{n}$, let the maximum likelihood estimates in the case every agent report truthfully be,
\begin{align*}
w^\star  &= \arg\displaystyle\max_{w} \displaystyle\sum_{i'=1}^{n} \log\left(\mathbb{P}\left\{Y = y_{i'} | x_{i'}, w\right\}\right).
\end{align*}
Fix an agent $i $, her feature vector $x_{i}$, and fix other agents' reports, $\left(x_{-i}, y_{-i}\right)$, define the maximum likelihood estimate given the misreport of agent $i , y^{\dagger}_{i}$ as,
\begin{align*}
w^{\dagger} &= \arg\displaystyle\min_{w} \displaystyle\sum_{i'=1, i' \neq  i}^{n} \log\left(\mathbb{P}\left\{Y = y_{i'} | x_{i}\left(i'\right), w\right\}\right) + \log\left(\mathbb{P}\left\{Y = y^{\dagger}_{i} | x_{i}, w\right\}\right).
\end{align*}
Now suppose, for a contradiction, that agent $i $ prefers misreporting, assume the following incentive inequality,
\begin{align*}
\mathbb{P}\left\{Y = y_{i} | x_{i}, w^\star \right\} &> \mathbb{P}\left\{Y = y_{i} | x_{i}, w^{\dagger}\right\}.
\end{align*}
If there are only two classes, then by symmetry,
\begin{align*}
\mathbb{P}\left\{Y = y^{\dagger}_{i} | x_{i}, w^\star \right\} &< \mathbb{P}\left\{Y = y^{\dagger}_{i} | x_{i}, w^{\dagger}\right\}.
\end{align*}
If there are more than two classes, fix a third $y'_{i} \notin \left\{y_{i}, y^{\dagger}_{i}\right\}$, and define an intermediate maximum likelihood estimate from removing the point $\left(x_{i}, y_{i}\right)$,
\begin{align*}
w' &= \arg\displaystyle\min_{w} \displaystyle\sum_{i'=1, i' \neq  i}^{n} \log\left(\mathbb{P}\left\{Y = y_{i'} | x_{i}\left(i'\right), w\right\}\right),
\end{align*}
then the Monotonic Condition implies,
\begin{align*}
\dfrac{\mathbb{P}\left\{Y = y_{i} | x_{i}, w^\star \right\}}{\mathbb{P}\left\{Y = y'_{i} | x_{i}, w^\star \right\}} &\leq  \dfrac{\mathbb{P}\left\{Y = y_{i} | x_{i}, w'\right\}}{\mathbb{P}\left\{Y = y'_{i} | x_{i}, w'\right\}},
\end{align*}
and the IIA Condition implies,
\begin{align*}
\dfrac{\mathbb{P}\left\{Y = y_{i} | x_{i}, w'\right\}}{\mathbb{P}\left\{Y = y'_{i} | x_{i}, w'\right\}} &= \dfrac{\mathbb{P}\left\{Y = y_{i} | x_{i}, w^{\dagger}\right\}}{\mathbb{P}\left\{Y = y'_{i} | x_{i}, w^{\dagger}\right\}}.
\end{align*}
Combining the above two inequalities with the incentive inequality, we have,
\begin{align*}
\mathbb{P}\left\{Y = y'_{i} | x_{i}, w^\star \right\} &> \mathbb{P}\left\{Y = y'_{i} | x_{i}, w^{\dagger}\right\}.
\end{align*}
Note that the above inequality is true for all $y'_{i} \notin \left\{y_{i}, y^{\dagger}_{i}\right\}$, summing over all such $y'_{i}$ results in,
\begin{align*}
\displaystyle\sum_{y \notin \left\{y_{i}, y^{\dagger}_{i}\right\}} \mathbb{P}\left\{Y = y'_{i} | x_{i}, w^\star \right\} &> \displaystyle\sum_{y \notin \left\{y_{i}, y^{\dagger}_{i}\right\}} \mathbb{P}\left\{Y = y'_{i} | x_{i}, w^{\dagger}\right\},
\end{align*}
given that the class probabilities sum up to $1$,
\begin{align*}
1 - \mathbb{P}\left\{Y = y_{i} | x_{i}, w^\star \right\} - \mathbb{P}\left\{Y = y^{\dagger}_{i} | x_{i}, w^\star \right\} &> 1 - \mathbb{P}\left\{Y = y_{i} | x_{i}, w^{\dagger}\right\} - \mathbb{P}\left\{Y = y^{\dagger}_{i} | x_{i}, w^{\dagger}\right\},
\end{align*}
and using the incentive inequality again,
\begin{align*}
\mathbb{P}\left\{Y = y^{\dagger}_{i} | x_{i}, w^\star \right\} &< \mathbb{P}\left\{Y = y^{\dagger}_{i} | x_{i}, w^{\dagger}\right\}.
\end{align*}
Now, due to the optimality of $h^{\dagger,}$
\begin{align*}
&  \displaystyle\sum_{i'=1, i' \neq  i}^{n} \log\left(\mathbb{P}\left\{Y = y_{i'} | x_{i'}, w^{\dagger}\right\}\right) + \log\left(\mathbb{P}\left\{Y = y^{\dagger}_{i} | x_{i}, w^{\dagger}\right\}\right)
\\ &\leq  \displaystyle\sum_{i'=1, i' \neq  i}^{n} \log\left(\mathbb{P}\left\{Y = y_{i'} | x_{i'}, w^\star \right\}\right) + \log\left(\mathbb{P}\left\{Y = y^{\dagger}_{i} | x_{i}, w^\star \right\}\right),
\end{align*}
using the above inequalities, the comparison can be simplified to,
\begin{align*}
\displaystyle\sum_{i'=1, i' \neq  i}^{n} \log\left(\mathbb{P}\left\{Y = y_{i'} | x_{i'}, w^{\dagger}\right\}\right) &< \displaystyle\sum_{i'=1, i' \neq  i}^{n} \log\left(\mathbb{P}\left\{Y = y_{i'} | x_{i'}, w^\star \right\}\right),
\\ \displaystyle\sum_{i'=1}^{n} \log\left(\mathbb{P}\left\{Y = y_{i'} | x_{i'}, w^{\dagger}\right\}\right) &< \displaystyle\sum_{i'=1}^{n} \log\left(\mathbb{P}\left\{Y = y_{i'} | x_{i'}, w^\star \right\}\right),
\end{align*}
which is a contradiction to the optimality of $w^\star $.
\newline \newline\end{proof}
A similar result can be obtained for empirical risk minimization. We could either add an assumption that the loss function can be normalized so that the sum is $1$ so it behaves the same way as a probabilistic classifier, or we could use stronger Monotonic and IIA Conditions.
\newline \newline
\bibliographystyle{te}
\bibliography{cs}


\end{document}
